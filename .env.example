# Node Environment
NODE_ENV=development

# Server Configuration
PORT=3000
HOST=localhost

# LLM Configuration
LLM_PROVIDER=local # Options: openai, mistral, local, ollama
OPENAI_API_KEY=your-openai-api-key
MISTRAL_API_KEY=your-mistral-api-key

# Local LLM Configuration
LOCAL_LLM_ENDPOINT=http://localhost:8080/v1
LOCAL_LLM_MODEL=llama2-7b
OLLAMA_ENDPOINT=http://localhost:11434
OLLAMA_MODEL=mistral:latest

# LLM Settings
LLM_MODEL=mistral-large
LLM_TEMPERATURE=0.7
LLM_MAX_TOKENS=2048
LLM_TIMEOUT=30000

# Database Configuration
REDIS_URL=redis://localhost:6379
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=your-neo4j-password
DATABASE_URL=postgresql://user:password@localhost:5432/vegapunk

# Communication Configuration
WEBSOCKET_PORT=3001
MESSAGE_QUEUE_URL=amqp://localhost

# Memory Configuration
MEMORY_CACHE_TTL=3600
MEMORY_MAX_SIZE=1000
LONG_TERM_MEMORY_THRESHOLD=0.8

# Agent Configuration
AGENT_CYCLE_INTERVAL=1000
AGENT_MAX_CONCURRENT_TASKS=10
AGENT_DECISION_TIMEOUT=5000

# Monitoring Configuration
METRICS_ENABLED=true
METRICS_PORT=9090
LOG_LEVEL=info
LOG_FORMAT=json

# Security Configuration
JWT_SECRET=your-jwt-secret
ENCRYPTION_KEY=your-encryption-key
API_RATE_LIMIT=100

# Feature Flags
ENABLE_LEARNING=true
ENABLE_AUTONOMOUS_MODE=true
ENABLE_DEBUG_MODE=false
ENABLE_LOCAL_INFERENCE=true