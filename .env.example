# ===============================================
# VEGAPUNK BACKEND ENVIRONMENT VARIABLES - EXAMPLE
# ===============================================
# Copy this file to .env and fill in your actual values

# ===============================================
# LLM PROVIDERS CONFIGURATION
# ===============================================

# Ollama (Local LLM)
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=qwen2:7b

# Hugging Face (Cloud LLM)
# Get your token from: https://huggingface.co/settings/tokens
HUGGING_FACE_API_KEY=hf_your_token_here
HUGGING_FACE_MODEL=microsoft/DialoGPT-medium

# ===============================================
# SERVER CONFIGURATION
# ===============================================

# Server Port
PORT=8080

# Environment Mode
NODE_ENV=development

# ===============================================
# SECURITY & AUTHENTICATION
# ===============================================

# JWT Secret (for future auth implementation)
JWT_SECRET=your-super-secret-jwt-key-change-this-in-production

# Session Secret
SESSION_SECRET=your-session-secret-change-this-in-production

# ===============================================
# EXTERNAL APIs (Optional)
# ===============================================

# OpenAI (if you want to add OpenAI support)
OPENAI_API_KEY=sk-your-openai-key-here

# Mistral AI (if you want to add Mistral support)
MISTRAL_API_KEY=your-mistral-key-here

# ===============================================
# DATABASE (For future features)
# ===============================================

# Database URL (if/when we add database)
DATABASE_URL=postgresql://user:password@localhost:5432/vegapunk

# ===============================================
# LOGGING & MONITORING
# ===============================================

# Log Level (error, warn, info, debug)
LOG_LEVEL=info

# Log File Path
LOG_FILE=./logs/vegapunk.log

# ===============================================
# PERFORMANCE & LIMITS
# ===============================================

# Chat History Limit
MAX_CHAT_HISTORY=50

# Request Timeout (milliseconds)
REQUEST_TIMEOUT=120000

# Max Tokens for LLM Responses
MAX_TOKENS=2048

# ===============================================
# CORS CONFIGURATION
# ===============================================

# Allowed Origins for CORS
CORS_ORIGINS=http://localhost:3000,http://localhost:5173

# ===============================================
# DEVELOPMENT SETTINGS
# ===============================================

# Enable Debug Mode
DEBUG_MODE=true

# Hot Reload for Models
HOT_RELOAD_MODELS=true